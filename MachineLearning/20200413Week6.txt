上交ACM班 《机器学习》  12:55——

卷积神经网络

下采样池化层：
优点：减少计算；是一种获取给定兴趣区域相应最强烈的节点
缺点：丢失信息


循环神经网络

前馈神经网络是一个有向无环图，无自连边
循环神经网络有一条自连的边，将隐层信息随着时间步往前传


树模型基础

树模型：
 - 中间节点用于分割数据
 - 叶子节点用于标签预测

树模型的核心问题：
 - 如何选择分裂节点的条件？
 - 如何做出预测？
 - 如何决定树结构？

1.决策树
 - 决策树将特征空间分割成与坐标轴平行的（超）矩形
 - 每一个矩形区域由一个标签或者不同的标签的概率分布所标记

如何选择分裂节点的条件？ —— 选择具有更强分类能力的特征（量化地说，即具有更高的信息增益的特征）

信息论基础
 - 香农熵（entropy）的含义是：接收的每条消息中包含的信息的平均量。
 - 熵越大，不确定性越大。

 - 交叉熵，用来度量两个随机变量分布之间的差异 

 - KL散度（也称相对熵，是两个概率分布之间差别的非对称性的度量）

 - 条件熵，在给定条件Y下，计算X的熵。可以得到给定条件Y下，X的信息增益

 - 信息增益，I(X, Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)

 - 信息增益率，信息增益与熵之间的比值（可以限制不单纯通过将数据分得很散从而得到较高信息增益率的情况）



ID3决策树 （一个特征最多出现在一条路径中一次）

- 从包含所有数据的根节点开始：
  - 对于每一个节点，计算所有可能特征带来的信息增益
  - 选择具有最大信息增益的特征
  - 根据选择的特征分割这一节点上的数据
 
- 对每个叶子节点递归进行上述步骤，直至：
  - 叶子节点不再有信息增益
  - 没有更多的特征可以被选取

通过对每一个样例生成一个叶子节点，树模型可以近似任何有限数据（发生过拟合）

目标函数（寻找一棵树能够最小化损失函数）

ID3算法总结
 - ID3算法是一种传统并且直观的决策树训练算法
  -- 针对离散的类别型数据
  -- 一个特征数值或类别构成一个分支

 - C4.5算法与ID3算法相似但是C4.5算法做了改进
  -- 根据信息增益率进行节点分裂 

 - 分裂出来的树枝数目取决于特征中不同类别取值的数量
  -- 可能会产生很宽的树结构



