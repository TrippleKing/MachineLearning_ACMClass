上交ACM班  《机器学习》  12:55——

CART算法（Classification and Regression Tree）
 - 二值分裂，对于分裂条件仅取yes或者no
 - 对于连续数值特征同样有效
 - 当采取不同分裂条件时，可以重复使用相同的特征


回归树（针对回归问题）
 - 输出预测数值
    优化目标：叶子节点的均方误差

如何得到最优的分隔区域？

如何寻找最优的分隔条件？

如何高效寻找最优阈值？（枚举法？先升序排序再找阈值）


分类树（针对分类问题）
 - 输出预测类别
    优化目标：叶子节点的基尼不纯度

如何得到最优的分割区域？

如何寻找最优的分割条件？

分割停止条件：
 1. 节点样例数很少；2. 基尼不纯度很小；3. 没有多余的特征


集成学习概念及应用

考虑一组预测模型f_1,...,f_L
 - 不同的预测模型在数据上具有不同的表现 

成功的集成需要多样性：预测模型应该犯的错误不相同；鼓励不同类型的预测模型集成在一起

组合模型：
1. 简单平均；（回归问题求平均值，分类问题进行投票）
2. 带权平均；
3. 门控（设计不同的可学习门控功能）；
4. 树模型（用决策树作为集成模型）

成功的集成学习需要多样性：
 1. 单个预测模型可能会犯不同的错误
 2. 一些多样性策略：
   - 包含不同类型的预测模型
   - 改变训练集
   - 改变特征集



Bagging算法（Bootstrap aggregating）

自助复制（Bootstrap）
 - 对于一个有n个训练样本的训练集z，通过有放回地采样n个样本得到一个新的训练集Z
 - 不包括大约37%的训练实例

Bagging
自助聚集算法（Bootstrap aggregating, Bagging）
 - 创建训练集的自助复制训练集
 - 为每个复制集训练预测模型
 - 使用非自助采样的数据验证预测模型
 - 对所有预测模型的输出取平均

留一自助法（Leave-One-Out Bootstrap）
当构建自助复制集时不采样示例i，然后利用示例i来评估模型

为什么Bagging算法有效？
- 偏差-方差分解

Bagging有效的原因是与原始模型相比偏差相同但是降低了方差（在整个数据集上进行训练）
 - 对低偏差和高方差的预测模型尤其有效


 
随机森林
随机森林与Bagging算法的主要区别在于构建了一个解耦合的树，然后对结果取平均
（每次在split point时只随机选择一部分特征）
在每个树节点分裂前，随机选择m<=p个变量作为分裂的候选变量（一般m=sqrt(p)，甚至取值为1）


Bagging vs 随机森林 vs Boosting
 - Bagging仅仅是在有相同权重的自助采样集上对每个预测模型进行训练
 - 随机森林通过采样特征的方法尝试去解耦合自助训练预测模型（决策树）
 - Boosting算法基于之前的预测模型有意识地学习和结合接下来的预测模型


加性模型（Additive Models）


AdaBoost
构造目标函数  J(F)=E[exp(-yF(x))]
几乎等价于逻辑交叉熵损失函数


提升算法的简史
 - 1990年-Schapire表明，弱学习器总是可以通过在输入数据的过滤版本上训练另外两个分类器来提高其性能
    -- 弱学习器是一种生成两元分类器的算法，其性能保证（以高概率）明显优于随机分类
 - 特别地：
    -- 在有N个样本的原始数据上训练分类器h_1
    -- 然后在一组新的N个样本上训练分类器h_2，其中有一半样本是被h_1错误分类的
    -- 然后在h_1和h_2分类结果不一致的N个样本上训练h_3
    -- 提升分类器h_B=多数投票（h_1,h_2,h_3）
    -- 事实证明，h_B比h_1性能更高

 - 1995年-Freund提出了一种“多数提升”的变种方法，将许多弱学习器同时结合起来并改善了Schapire简单提升算法的性能
    -- 这两种算法都要求弱学习器有固定的错误率

 - 1996年-Freund和Schapire提出了AdaBoost
    -- 去除了固定错误率的要求

 - 1996~1998 - Freund,Schapire和Singer以泛化误差的上界形式提出了一些理论来支持他们的算法
    -- 但是边界太松散而不具有实际意义
    -- 提升算法的实际表现远远超过了理论边界


GBDT梯度提升决策树
- 决策树的提升算法
  -- f(x)是一个决策树模型
  -- 许多变体，诸如GBRT, boosted trees, GBM

使用二阶泰勒展开

树的分裂并没有着眼最大化基尼不纯度或信息增益


XGBoost
