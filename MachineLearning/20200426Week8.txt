上交ACM班《机器学习》  12:55——

深度模型的一种解释
 - 增加模型复杂度→增强学习能力
  -- 增加隐层单元数（模型宽度）
  -- 增加隐层的层数（模型深度）

增加隐层的层数更为有效：
     --- 增加了具有激活函数的单元数
     --- 增加了函数的嵌入深度


 - 增加模型复杂度→加大过拟合风险以及训练难度
  -- 针对过拟合：提供大量训练数据
  -- 针对训练：提供高效的算力机器

当经过多层的反向传播，误差的梯度将发散，收敛到稳定状态将会非常困难：
     ---- 传统的反向传播算法将会失效，需要使用许多技巧进行训练


深度学习的成功：
  -- 庞大的训练数据：
    --- 减小过拟合风险最为简单有效的方式

  -- 高效的算力机器
    --- 针对大模型，没有GPU、TPU的加速，深度学习无法如此成功

  -- 训练技巧
    --- 启发式，甚至是无法解释的
    --- 使用传统的反向传播算法，误差的梯度将会在多层传播以后发散，很难收敛到稳定状态

深度学习的本质→表征学习（Representation Learning）

深度模型的关键：1. 逐层处理；2. 特征变换；3. 拥有足够的模型复杂度


深度森林
 - 用堆叠森林模型来实现：逐层处理、特征变换、足够的模型复杂度


多粒度级联森林（gcForest）
 - 一种决策树森林（集成学习）方法
 - 与DNNs相比，在多个任务上表现都具有很强竞争力
 - 超参数减少明显
   -- 更加容易设定参数
   -- 默认设定在多个任务上都可以很有效
 - 自适应的模型复杂度
   -- 根据数据集自动决定
   -- 可以在小数据集上应用

根据误差-分歧分解：集成误差=平均个体误差-平均个体分歧

单个学习模型越准确越具有差异，集成模型表现越佳

注意：
   -- “分歧”并没有一个操作性定义
   --  误差-分歧分解是在使用平方损失的回归问题中推导得到的

多粒度级联森林：
  -- 基本思想：加入更多的随机性
  -- 主要策略：
     --- 数据采样的处理
           -- Bagging中的自助采样
           -- Boosting中的重要性采样
     --- 输入特征的处理
           -- 在随机子空间中进行特征采样
     --- 可学习参数的处理
           -- NN的随机初始化Random Initialization
           -- 负相关学习Negative Correlation
     --- 输出表示的处理
           -- ECOC
           -- 输出翻转Flipping Output

上述策略并非一直有效：数据采样的处理对于“静态学习器”如线性分类器，SVM并不生效

级联森林结构：将多片森林的输出作为新的输入，传入另一层的多片森林：
   -- 类似于Stacking
   -- Stacking一般只对一到两层进行，多了容易过拟合，其并不能应用于深度模型中
采用不同种类的森林：鼓励集成模型的集成时的多样性

滑动窗口扫描：受到CNN/RNN的启发，挖掘空间及序列的关联

多粒度：不同大小的滑动窗口可以产生不同大小的采样数据




