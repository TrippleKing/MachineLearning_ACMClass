上交《机器学习》 12:55——

布置课程大作业（三选一）NLP、时空序列预测任务、强化学习任务

布置课程小作业SVM


激活函数与损失函数

激活函数：Sigmoid；Tanh；ReLU

Sigmoid函数，输出范围[0,1]，受生物神经元的启发产生。

Tanh函数，负值较大的输入经过tanh函数会映射为负输出，

ReLU函数，x增加时ReLU的梯度不会消失，计算速度快，


深度学习思想简介

深度学习是一种有着多个层次的表征学习方式。

通过简单但非线性的模块自动地将每个层次的特征（从原始输入开始）转变为更高级抽象的表征形式

主要通过神经网络实现


训练深度网络的难点：
1.缺乏大数据（现在我们有很多数据）
2.缺乏计算资源（现在有GPU和HPC）
3.容易进入（坏的）局部最小值（现在可以使用预训练技术和各种优化算法）
4.梯度消失（现在可以使用ReLU,批标准化，残差网络等）
5.正则化（现在可以使用dropout方法）

梯度消失问题的解决方法：
1.ReLU激活函数
2.深度残差网络
3.批标准化（Batch Normalization）

陷入局部最小的解决方法：
1.深度信念网络（DBN）
2.Adam优化算法

预训练：寻找一个好的网络初始化

限制玻尔兹曼机（RBM）是一种通过输入数据集学习概率分布的随机生成神经网络。


深度学习的正则化
1.Dropout
在训练阶段，每个小批量训练时随机禁用网络中一部分节点及其连接











