上交ACM班《机器学习》   12:55——

支持向量机优化求解

引入拉格朗日函数，转换为拉格朗日对偶形式（需要满足KKT条件）


在最初支持向量机的推导过程中，数据被假定位线性可分

但在实际场景中，数据往往是线性不可分的。

处理不可分情况：
 -- 增加松弛变量（能够解决一部分线性不可分，但不能解决高度非线性问题）

序列最小优化算法（SMO）
 -- 坐标上升法（固定一个特征，优化另一个特征，循环交替）
（注意：在梯度下降法中特征是同时被更新）

 -- 序列最小优化（SMO）算法


支持向量机核方法（解决高度线性不可分问题）

将特征向量映射到高维空间中，定义特征映射函数φ(x)，引入核函数K（如常用的高斯核函数，也被称为径向基函数RBF核），核矩阵K必定是对称矩阵，也是半正定矩阵。

Mercer定理，有效核的核矩阵都是半正定矩阵。

有效核举例：RBF核、多项式核、预先相似度核

Sigmoid核，神经网络使用sigmoid函数作为激活函数，使用sigmoid核的支持向量机相似与一个二层感知机。


广义线性模型（复习）
 -- 同样是引入映射函数，以及核函数


总结支持向量机

 -- 支持向量机是一种线性模型，优化目标是最大化决策边界距离数据点的最小距离，由此获得更好的分类鲁棒性

 -- 支持向量机的原问题可以转化为一个二次函数优化问题，最终由序列最小化算法来高效求解 

 -- 当对原始特征数据做映射变换，支持向量机可以被看成一个泛线性模型，而使用核方法可以让研究者仅仅关注定义两个数据点之间的相似性

--------------------------------------------------

人工神经网络

普适逼近定理
 -- 一个具有至少一层隐藏层的前馈神经网络，并且隐藏层包含有限数量的神经元（即多层感知机），它可以以任意精度逼近任意一个定义在R^n的闭集里的连续函数。


反向传播算法

 -- 输入数据逐层计算得到输出
 -- 将输出与正确答案比较得到误差，然而反向求导数
